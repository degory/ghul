# Tests

## Test framework

There is no in-process unit testing framework, because a compiler bug could easily bring the whole test process down catastrophically, both code under test and the framework itself.

Instead the test scripts capture warnings and errors generated by the compiler as each test is built, capture the output of the test executable as it runs, then compare the results against previously captured expected output.

## Test cases

Each of the folders in the `test/cases` folder constitutes a single test case and holds all the information needed to execute the test.

### Test case names

Test cases are named based on the directory they sit in under `test/cases`. For example `test/cases/fibonacci` holds the `fibonacci` test case. Test case names should be in `kebab-case`: all lower case, no spaces, and words separated by dashes.

### Test runner

The test runner is a ghul application (`tester`) which executes one or more test cases and collects and reports the results. Because the test runner needs to be run in a Docker container, you don't generally run it directly - see below for how to execute tests from Visual Studio Code and from the command line

### Test case execution

When a test case is executed, the test runner compares what actually happens ('outputs') against what should happen ('expectations'). If any outputs don't match what's expected, the test is failed.

When asked to execute a test case the test runner will:

- Run the ghul compiler from the root of the repository (so for the tests results to be meaningful, the compiler should have been built from current sources)
- Pass the compiler any source files and options configured by the test case
- Capture any errors and warnings produced by the compiler, compare them against the expected error and warning output, and fail the test if the actual output differs from the expected
- Check if the compiler produced an executable output and fail the test if either an executable was expected but none was produced, or if no executable was expected but one was in fact produced.

If the compilation produces an executable, then the test runner will also

- Run the resulting executable and capture its standard output
- Compare that output to the expected run output as specified in the test config and fail the test if it differs

### Test case files

The test runner is driven by various config, source, and expectation files in the test case folder.

#### Source files

- `*.ghul` (required) all ghul source files found in the test case folder are assumed to comprise a single test and are passed to the ghul compiler to build

#### Config

- `disabled` (optional) if this file is present in a test case folder, then the test runner will silently ignore the test case
- `ghulflags` (required) contains any command line options to pass to the ghul compiler
- `fail.expected`: (optional) if present then the test is not expected to produce a binary - if the test run produces a binary then the test runner will report the test as failed (and vice versa)
- `err.expected`: (required) expected compiler error output
- `warn.expected`: (required) expected compiler warning output
- `run.expected`: (required unless fail expected) expected built executable standard output

#### Outputs

- `failed`: created if a test fails/removed if it passes.
- `*.out`: actual outputs from the compiler and the built executable, if any
- `*.sort`: sorted versions of the outputs
- `*.diff`: differences between expected outputs and actual outputs

## Working with tests

Writing a test involves:

- Creating a new empty test from the test template
- Designing a test scenario that will exercise the code you want to test
- Writing code to actually execute that scenario
- Ensuring that a successful test run will deterministically produce the same output from the compiler and from any built executable on every test run
- Also ensuing any test failures will produce different output!
- Finally, capturing the expected test output as expectation files

This process can be managed from Visual Studio Code or from from the command line.

### Visual Studio Code

Each test case is a mini ghul project in its own right and can be opened as a project folder in Visual Studio Code individually. It's generally better to open a test case in a separate Visual Studio Code instance, rather than editing its files from VSCode alongside the compiler (because having files open from multiple different ghul projects in the same VSCode instance can result in confusing/misleading messages from the ghul language extension).

#### Creating a new test case

With the ghul compiler folder open in Visual Studio Code, run the create test task:

`<Ctrl>+<Shift>+P` | `Tasks: Run task` | `Create new test`

Then enter a kebab-case name for the new test when prompted in the terminal window.

A new VSCode window will open pointing at the new test project

#### Running a test case

With a test case folder open in Visual Studio Code, run the default test task to execute the test:

`<Ctrl>+<Shift>+P` | `Tasks: Run task` | `Run test`

The test results will appear in the terminal window

**Note** VSCode has no standard key binding for running the default test, but you can configure a custom binding if you want to access this task more easily

#### Capturing test case expectations

Once you have a test case that generates the appropriate output, you need to capture that output as expectation files. If on a future test run the test produces different output to what was expected, the test runner will flag the test as failed

With a test case folder open in Visual Studio Code, run the default build task to execute the test:

`<Ctrl>+<Shift>+P` | `Tasks: Run task` | `Capture test expectations`

**Note** If you're using the standard VSCode key bindings you can also do this via `<Ctrl>+<Shift>+B`

### Command line

You can also run tests, capture test expectations and create new tests from the command line.

#### Creating a new test case

To create a new test case run `./test/create.sh` and enter a kebab-case name for the new test case when prompted

#### Running all tests

To run all tests, from the repo root directory run `./test/test.sh`

#### Running a specific test

To run a specific test run `./test/test.sh test-case-name`, where `test-case-name` is the name of the directory under test/cases containing that test's files

#### Capturing test case expectations

To capture expected test result for a test case, from the repo root directory run `./test/capture.sh test-case-name`

**Note** a test must have previously been run and have produced output before that output can be captured as the expected result)

**Note** the current working directory must be the repository root when running these scripts
