namespace Syntax.Parser is
    use System;
    use Generic;

    use Logging;
    use Source;
    
    class CONTEXT is
        last_error_location: LOCATION;
        last_error_message: String;
        speculative_parse_tokens: Vector[Lexical.TOKEN_PAIR];
        allow_tuple_element: bool public;
        in_trait: bool public;
        tokenizer: Lexical.TOKENIZER;
        logger: Logger;

        location: LOCATION => tokenizer.current.location;

        is_end_of_file: bool => tokenizer.is_end_of_file;

        current: Lexical.TOKEN_PAIR => tokenizer.current;

        current_token: Lexical.TOKEN => tokenizer.current.token;

        current_string: String => tokenizer.current.string;

        current_token_name: String is
            var result = Lexical.TOKEN_NAMES[current_token];

            if result == null then
                result = "unknown";
            fi

            if current_string!=null && current_string!~result then
                result = result + ' ' + current_string;
            fi

            return result;
        si        

        init(tokenizer: Lexical.TOKENIZER, logger: Logger) is
            self.tokenizer = tokenizer;
            self.logger = logger;
            self.tokenizer.next();
            last_error_location = self.location;
            last_error_message = "";
        si

        write_token(token: Lexical.TOKEN_PAIR) is
            tokenizer.write_token(token);
            next_token();
        si

        write_tokens(tokens: Iterable[Lexical.TOKEN_PAIR]) is
            tokenizer.write_tokens(tokens);
            next_token();
        si

        mark() is
            assert(speculative_parse_tokens == null, "cannot start nested speculative parse");
            speculative_parse_tokens = new Vector[Lexical.TOKEN_PAIR]();
            speculative_parse_tokens.add(current);
        si

        commit() is
            assert(speculative_parse_tokens != null, "cannot commit: not in speculative parse");
            speculative_parse_tokens = null;
        si

        backtrack() is
            assert(speculative_parse_tokens != null, "cannot backtrack: not in speculative parse");
            write_tokens(speculative_parse_tokens);
            speculative_parse_tokens = null;
        si

        next_token() -> bool is
            var result = tokenizer.next();

            if speculative_parse_tokens != null then
                speculative_parse_tokens.add(current);
            fi

            return result;
        si

        next_token(token: Lexical.TOKEN, message: String) -> bool is
            if expect_token(token, message) then
                next_token();
                return true;
            fi
        si

        next_token(tokens: List[Lexical.TOKEN], message: String) -> bool is
            if expect_token(tokens, message) then
                next_token();
                return true;
            fi
        si

        next_token(token: Lexical.TOKEN) -> bool is
            return next_token(token, "syntax error");
        si

        next_token(tokens: List[Lexical.TOKEN]) -> bool is
            return next_token(tokens, "syntax error");
        si        

        location_and_next() -> LOCATION is
            var result = location;
            next_token();
            return result;
        si

        skip_token(tokens: List[Lexical.TOKEN], message: String) is
            var start = location;
            
            do
                if is_end_of_file then
                    error(start..location, "%: expected %" % [message, Lexical.TOKEN_NAMES[current_token]]: Object );
                    return;
                elif tokens.contains(current_token) then
                    error(start..location, "%: expected %" % [message, Lexical.TOKEN_NAMES[current_token]]: Object );
                    next_token();
                    return;
                else
                    next_token();
                fi
            od
        si

        skip_token(token: Lexical.TOKEN, message: String) is
            var t = new Vector[Lexical.TOKEN]();
            t.add(token);
            skip_token(t, message);
        si

        expect_token(token: Lexical.TOKEN, message: String) -> bool is
            if current_token != token then
                error(location, "%: expected % but found %" % [message, Lexical.TOKEN_NAMES[token], current_token_name]: Object );
                return false;
            else
                return true;
            fi
        si

        expect_token(token: Lexical.TOKEN) -> bool is
            return expect_token(token, "syntax error");
        si

        expect_token(tokens: List[Lexical.TOKEN], message: String) -> bool is
            if !tokens.contains(current_token) then
                error(location, "%: expected % but found %" % [message, Lexical.TOKEN_NAMES[tokens], current_token_name]: Object );
                return false;
            else
                return true;
            fi
        si

        expect_token(tokens: List[Lexical.TOKEN]) -> bool is
            return expect_token(tokens, "syntax error");
        si

        error(location: LOCATION, message: String) is
            if location!~last_error_location || message!~last_error_message then
                last_error_location = location;
                last_error_message = message;
                logger.error(location, message);
            elif !is_end_of_file then
                next_token();
            else
                throw new Exception("give up");
            fi
        si
    si
si
