namespace Syntax.Parsers is
    use IO.Std;
    
    use Logging;
    use Source;
    
    class CONTEXT is
        _repeated_error_count_at_eof: int;

        last_error_location: LOCATION;
        last_error_message: string;
        speculative_parse_tokens_stack: Collections.STACK[Collections.MutableList[Lexical.TOKEN_PAIR]];
        speculative_parse_tokens: Collections.MutableList[Lexical.TOKEN_PAIR] =>
            if speculative_parse_tokens_stack.count > 0 then
                speculative_parse_tokens_stack.peek();
            else
                null
            fi;

        allow_tuple_element: bool public;
        in_trait: bool public;

        in_global_function: bool public;
        in_classy: bool public;
        in_member: bool public;

        // intentation is not part of the syntax, but it is used for
        // for error recovery: we may want to skip over a block of code
        // out to a given indentation level
        global_definition_indent: int public;
        method_indent: int public;

        tokenizer: Lexical.TOKENIZER;
        logger: Logger;

        location: LOCATION => tokenizer.current.location;

        is_end_of_file: bool => tokenizer.is_end_of_file;

        current: Lexical.TOKEN_PAIR => tokenizer.current;

        current_token: Lexical.TOKEN => tokenizer.current.token;

        current_string: string => tokenizer.current.value_string;

        current_token_name: string is
            let result = Lexical.TOKEN_NAMES[current_token];

            if result == null then
                result = "unknown";
            fi

            if current_string? /\ current_string !~ result then
                result = result + ' ' + current_string;
            fi

            return result;
        si        

        init(tokenizer: Lexical.TOKENIZER, logger: Logger) is
            speculative_parse_tokens_stack = new Collections.STACK[Collections.MutableList[Lexical.TOKEN_PAIR]]();

            self.tokenizer = tokenizer;
            self.logger = logger;

            self.tokenizer.next();

            last_error_location = self.location;
            last_error_message = "";
        si

        prev_token(token: Lexical.TOKEN_PAIR) is
            tokenizer.prev_token(token);

            next_token();
        si

        prev_token(tokens: Collections.Iterable[Lexical.TOKEN_PAIR]) is
            tokenizer.prev_tokens(tokens);

            next_token();
        si

        speculate() is
            let new_speculative_parse_tokens = new Collections.LIST[Lexical.TOKEN_PAIR]();
            new_speculative_parse_tokens.add(current);

            speculative_parse_tokens_stack.push(new_speculative_parse_tokens);
        si

        commit() is
            assert speculative_parse_tokens_stack.count > 0 else "cannot commit: not in speculative parse";

            Logging.debug("committing speculative parse: {speculative_parse_tokens_stack.peek() | }");

            // we know we won't need to backtrack, so we can throw away the speculative parse tokens
            speculative_parse_tokens_stack.pop();
        si

        backtrack() is
            assert speculative_parse_tokens_stack.count > 0 else "cannot backtrack: not in speculative parse";

            // get the tokens we speculatively read ready to be read again
            let tokens_to_rewind = speculative_parse_tokens_stack.pop();

            Logging.debug("backtracking speculative parse: {tokens_to_rewind | }");

            // push the tokens we speculatively read back so they can be read again - they either go
            // into the next speculative parse, or they are read again in the normal parse
            prev_token(tokens_to_rewind);
        si

        next_token() -> bool is
            let result = tokenizer.next();

            if speculative_parse_tokens_stack.count > 0 then
                speculative_parse_tokens_stack.peek().add(current);
            fi

            return result;
        si

        next_token(token: Lexical.TOKEN, message: string) -> bool is
            if expect_token(token, message) then
                next_token();
                return true;
            fi
        si

        next_token(tokens: Collections.List[Lexical.TOKEN], message: string) -> bool is
            if expect_token(tokens, message) then
                next_token();
                return true;
            fi
        si

        next_token(token: Lexical.TOKEN) -> bool is
            return next_token(token, "syntax error");
        si

        next_token(tokens: Collections.List[Lexical.TOKEN]) -> bool is
            return next_token(tokens, "syntax error");
        si        

        location_and_next() -> LOCATION is
            let result = location;
            next_token();
            return result;
        si

        skip_token(tokens: Collections.List[Lexical.TOKEN], message: string) is
            let start = location;
            
            do
                if is_end_of_file then
                    error(start::location, "{message}: expected {Lexical.TOKEN_NAMES[current_token]}");
                    return;
                elif tokens | .any(t => t == current_token) then
                    error(start::location, "{message}: expected {Lexical.TOKEN_NAMES[current_token]}");
                    next_token();
                    return;
                else
                    next_token();
                fi
            od
        si

        skip_token(token: Lexical.TOKEN, message: string) is
            let t = new Collections.LIST[Lexical.TOKEN]();
            t.add(token);
            skip_token(t, message);
        si

        expect_token(token: Lexical.TOKEN, message: string) -> bool is
            if current_token != token then
                if current_token != Lexical.TOKEN.CANCEL_STRING then
                    error(location, "{message}: expected {Lexical.TOKEN_NAMES[token]} but found {current_token_name}");
                fi

                return false;
            else
                return true;
            fi
        si

        expect_token(token: Lexical.TOKEN) -> bool is
            return expect_token(token, "syntax error");
        si

        expect_token(tokens: Collections.List[Lexical.TOKEN], message: string) -> bool is
            if !tokens | .any(t => t == current_token) then
                if current_token != Lexical.TOKEN.CANCEL_STRING then
                    error(location, "{message}: expected {Lexical.TOKEN_NAMES[tokens]} but found {current_token_name}");
                fi

                return false;
            else
                return true;
            fi
        si

        expect_token(tokens: Collections.List[Lexical.TOKEN]) -> bool is
            return expect_token(tokens, "syntax error");
        si

        warn(location: LOCATION, message: string) is
            logger.warn(location, message);
        si

        error(location: LOCATION, message: string) is
            if location !~ last_error_location \/ message !~ last_error_message then
                _repeated_error_count_at_eof = 0;

                last_error_location = location;
                last_error_message = message;
                // IO.Std.error.write_line("PARSER error {message}");
                logger.error(location, message);
            elif !is_end_of_file then
                next_token();
            else
                _repeated_error_count_at_eof = _repeated_error_count_at_eof + 1;

                if (_repeated_error_count_at_eof > 10) then
                    throw new Compiler.PARSE_EXCEPTION("repeated errors at end of file");
                fi
            fi
        si
    si
si
