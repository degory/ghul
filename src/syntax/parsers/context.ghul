namespace Syntax.Parsers is
    use IO.Std;
    
    use Logging;
    use Source;

    struct TOKEN_LOOKAHEAD_SPECULATE_THEN_COMMIT: Disposable is
        _context: CONTEXT;

        is_speculating: bool => _context != null;

        init(context: CONTEXT) is
            _context = context;
            _context.tokenizer_speculate();
        si

        backtrack() is
            _context.tokenizer_backtrack();
            _context = null;
        si

        backtrack_if_speculating() is
            if _context != null then
                _context.tokenizer_backtrack();
                _context = null;
            fi
        si

        commit() is
            _context.tokenizer_commit();
            _context = null;
        si

        commit_if_speculating() is
            if _context != null then
                _context.tokenizer_commit();
                _context = null;
            fi
        si
        
        cancel() is
            _context = null;
        si        

        dispose() is
            if _context != null then
                _context.tokenizer_commit();
                _context = null;
            fi
        si
    si

    struct TOKEN_LOOKAHEAD_SPECULATE_THEN_BACKTRACK: Disposable is
        _context: CONTEXT;

        is_speculating: bool => _context != null;

        init(context: CONTEXT) is
            _context = context;
            _context.tokenizer_speculate();
        si

        backtrack() is
            _context.tokenizer_backtrack();
            _context = null;
        si

        backtrack_if_speculating() is
            if _context != null then
                _context.tokenizer_backtrack();
                _context = null;
            fi
        si

        commit() is
            _context.tokenizer_commit();
            _context = null;
        si

        commit_if_speculating() is
            if _context != null then
                _context.tokenizer_commit();
                _context = null;
            fi
        si

        cancel() is
            _context = null;
        si        

        dispose() is
            if _context != null then
                _context.tokenizer_backtrack();
                _context = null;
            fi
        si
    si

    class CONTEXT is
        _repeated_error_count_at_eof: int;

        last_error_location: LOCATION;
        last_error_message: string;

        allow_tuple_element: bool public;
        in_trait: bool public;

        in_global_function: bool public;
        in_classy: bool public;
        in_member: bool public;

        // intentation is not part of the syntax, but it is used for
        // for error recovery: we may want to skip over a block of code
        // out to a given indentation level
        global_indent: int public;
        member_indent: int public;

        tokenizer: Lexical.TOKEN_LOOKAHEAD;
        logger: Logger;

        location: LOCATION => current.location;

        is_end_of_file: bool;

        current: Lexical.TOKEN_PAIR;

        current_token: Lexical.TOKEN => current.token;

        current_string: string => current.value_string;

        current_token_name: string is
            let result = Lexical.TOKEN_NAMES[current_token];

            if result == null then
                result = "unknown";
            fi

            if current_string? /\ current_string !~ result then
                result = result + ' ' + current_string;
            fi

            return result;
        si        

        init(tokenizer: Lexical.TOKEN_LOOKAHEAD, logger: Logger) is
            // speculative_parse_tokens_stack = new Collections.STACK[Collections.MutableList[Lexical.TOKEN_PAIR]]();

            self.tokenizer = tokenizer;
            self.logger = logger;

            current = tokenizer.read_token();

            last_error_location = self.location;
            last_error_message = "";
        si

        tokenizer_speculate_then_commit() -> TOKEN_LOOKAHEAD_SPECULATE_THEN_COMMIT =>
            new TOKEN_LOOKAHEAD_SPECULATE_THEN_COMMIT(self);
        
        tokenizer_speculate_then_backtrack() -> TOKEN_LOOKAHEAD_SPECULATE_THEN_BACKTRACK =>
            new TOKEN_LOOKAHEAD_SPECULATE_THEN_BACKTRACK(self);

        logger_speculate_then_commit() -> LOGGER_SPECULATE_THEN_COMMIT => logger.speculate_then_commit();

        logger_speculate_then_backtrack() -> LOGGER_SPECULATE_THEN_BACKTRACK => logger.speculate_then_backtrack();

        tokenizer_speculate() is
            tokenizer.speculate();
        si

        tokenizer_commit() is
            tokenizer.commit();
        si

        tokenizer_backtrack() is
            current = tokenizer.backtrack();
        si

        logger_speculate() is
            logger.speculate();
        si
        
        logger_commit() is
            logger.commit();
        si

        logger_backtrack() -> DIAGNOSTICS_STATE is
            return logger.roll_back();
        si
        
        expect_format_specifier() is
            tokenizer.expect_format_specifier();
        si

        next_token() -> bool is
            current = tokenizer.read_token();

            // if speculative_parse_tokens_stack.count > 0 then
            //     speculative_parse_tokens_stack.peek().add(current);
            // fi

            is_end_of_file = current.token == Lexical.TOKEN.END_OF_INPUT;

            return is_end_of_file;
        si

        next_token(token: Lexical.TOKEN, message: string) -> bool is
            if expect_token(token, message) then
                next_token();
                return true;
            fi
        si

        next_token(tokens: Collections.List[Lexical.TOKEN], message: string) -> bool is
            if expect_token(tokens, message) then
                next_token();
                return true;
            fi
        si

        next_token(token: Lexical.TOKEN) -> bool is
            return next_token(token, "syntax error");
        si

        next_token(tokens: Collections.List[Lexical.TOKEN]) -> bool is
            return next_token(tokens, "syntax error");
        si        

        location_and_next() -> LOCATION is
            let result = location;
            next_token();
            return result;
        si

        skip_token(tokens: Collections.List[Lexical.TOKEN], message: string) is
            let start = location;
            
            do
                if is_end_of_file then
                    error(start::location, "{message}: expected {Lexical.TOKEN_NAMES[current_token]}");
                    return;
                elif tokens | .any(t => t == current_token) then
                    error(start::location, "{message}: expected {Lexical.TOKEN_NAMES[current_token]}");
                    next_token();
                    return;
                else
                    next_token();
                fi
            od
        si

        skip_token(token: Lexical.TOKEN, message: string) is
            let t = new Collections.LIST[Lexical.TOKEN]();
            t.add(token);
            skip_token(t, message);
        si

        expect_token(token: Lexical.TOKEN, message: string) -> bool is
            if current_token != token then
                if current_token != Lexical.TOKEN.CANCEL_STRING then
                    error(location, "{message}: expected {Lexical.TOKEN_NAMES[token]} but found {current_token_name}");
                fi

                return false;
            else
                return true;
            fi
        si

        expect_token(token: Lexical.TOKEN) -> bool is
            return expect_token(token, "syntax error");
        si

        expect_token(tokens: Collections.List[Lexical.TOKEN], message: string) -> bool is
            if !tokens | .any(t => t == current_token) then
                if current_token != Lexical.TOKEN.CANCEL_STRING then
                    error(location, "{message}: expected {Lexical.TOKEN_NAMES[tokens]} but found {current_token_name}");
                fi

                return false;
            else
                return true;
            fi
        si

        expect_token(tokens: Collections.List[Lexical.TOKEN]) -> bool =>
            expect_token(tokens, "syntax error");

        warn(location: LOCATION, message: string) is
            logger.warn(location, message);
        si

        error(location: LOCATION, message: string) is
            if location !~ last_error_location \/ message !~ last_error_message then
                _repeated_error_count_at_eof = 0;

                last_error_location = location;
                last_error_message = message;

                logger.error(location, message);
            elif !is_end_of_file then
                next_token();
            else
                _repeated_error_count_at_eof = _repeated_error_count_at_eof + 1;

                if (_repeated_error_count_at_eof > 10) then
                    throw new Compiler.PARSE_EXCEPTION("repeated errors at end of file");
                fi
            fi
        si
    si
si
