namespace Lexical is
    use Collections;

    use Logging;

    trait TokenSource is
        read_token() -> TOKEN_PAIR;
    si

    class TOKEN_LOOKAHEAD is
        _queue: TOKEN_QUEUE;
        _mark_stack: STACK[(mark: int, prev_token: TOKEN_PAIR)];

        _tokenizer: TokenSource;

        init(tokenizer: TokenSource) is
            _queue = new TOKEN_QUEUE();
            _mark_stack = new STACK[(int, TOKEN_PAIR)]();

            _tokenizer = tokenizer;
        si

        // when we first start speculating, we need to tell
        // the token queue to mark the read position, because
        // we must not write past it
        speculate(prev_token: TOKEN_PAIR) is
            if _mark_stack.count == 0 then
                _queue.speculate_enter();
            fi

            // FIXME: we can get the token from the previous slot
            // in the queue
            _mark_stack.push((_queue.mark(), prev_token));
        si

        backtrack() -> TOKEN_PAIR is
            // we want to carry on reading tokens from the last
            // saved read position, effectively undoing the
            // speculation

            let mark_prev_token = _mark_stack.pop();

            _queue.release(mark_prev_token.mark);

            if _mark_stack.count == 0 then
                _queue.speculate_exit();
            fi

            // FIXME: we can get the token from the previous slot
            // in the queue
            return mark_prev_token.prev_token;
        si

        commit() is
            // we want to discard the saved read position, and
            // carry on reading tokens from the current position

            _mark_stack.pop();

            // if we have no more saved read positions, then we
            // can tell the token queue it's safe to write past
            // the speculative read position again
            if _mark_stack.count == 0 then
                _queue.speculate_exit();
            fi
        si

        read_token() -> TOKEN_PAIR is
            if !_queue.avail then
                let result = _tokenizer.read_token();

                _queue.enqueue(result);
            fi

            return _queue.dequeue(); 
        si
    si
si